{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d2405c-4cdb-4041-8ff7-b63183acf3bc",
   "metadata": {},
   "source": [
    "# Diffusion LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1311b5-0269-43ce-a0a8-c72193d2cbba",
   "metadata": {},
   "source": [
    "Current closed model: \n",
    "[https://chat.inceptionlabs.ai/](https://chat.inceptionlabs.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c03677f-5096-46b0-b0de-e1c07d0f451e",
   "metadata": {},
   "source": [
    "# Large Language Diffusion Models (LLaDA)\n",
    "\n",
    "_\"Large Language Diffusion Models\" by Nie, Zhu, et al. (arXiv:2502.09992)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a3ff0-1c52-437a-a566-aa0a24160c8e",
   "metadata": {},
   "source": [
    "Proposes that \"_Generative modeling principles_, rather than the autoregressive formulation [...] fundamentally underpin the essential properties of LLMs\".\n",
    "\n",
    "The paper introduces **masked diffusion models** (MDM), bidrectional BERT-like models that operate on entire sequences in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b721e731-b1db-49b3-8ddf-91345aaccf8b",
   "metadata": {},
   "source": [
    "Instead of predicting the next token, a diffusion LLM must \"de-mask\" a corrupted sequence of text.\n",
    "\n",
    "This is very similar to BERT! Except that we use a variable percentage of noise, rather than a fixed masking rate of 15%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f882db8-f4ef-4086-8964-3ea9d0578f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids[i] = (\n",
    "#     mask_id if dice < .8\n",
    "#     else random.randint(0, tok.vocab_size - 1) if dice < .9\n",
    "#     else ids[i]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c34332-7e89-4f33-bf70-27df53d4c1a7",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "A major disadvantage for all \"BERT-like\" models -- you only get signal when training on a small portion of the masked tokens.\n",
    "\n",
    "```\n",
    "The [MASK] of France [MASK] Paris [EOS]\n",
    "       ^                ^\n",
    "  Only the masked tokens contribute to the loss\n",
    "```\n",
    "\n",
    "> For chosen tokens, BERT masks the token 80% of the time, adds a random vocab 10%, and keeps the token the same 10% of the time.\n",
    "\n",
    "This is a real disadvantage, in autoregressive models every token predicts the next, and so all contribute to the loss.\n",
    "\n",
    "This isn't possible in BERT like models since we have no attention masking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fad65c-b6a3-4c57-bc0d-7b0630f1257b",
   "metadata": {},
   "source": [
    "### A Minimal Training Script\n",
    "\n",
    "Given a sequence of text, \"corrupt\" a variable percentage with `[MASK]`. The model is trained to predict the original tokens that were masked with CEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa45631-8d26-4bc4-8716-70558602ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, torch\n",
    "from datasets import load_dataset\n",
    "from accelerate import notebook_launcher\n",
    "from transformers import (AutoTokenizer, AutoModelForMaskedLM,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer)\n",
    "def main():\n",
    "    model_id = \"answerdotai/ModernBERT-large\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "    mask_id, sep_id, sep = tok.mask_token_id, tok.sep_token_id, tok.sep_token\n",
    "    tok.chat_template = (\n",
    "        \"User: {{ messages[0]['content'] }}\\n\" + sep + \"\\nAssistant:\\n{% generation %}{{ messages[1]['content'] }}{% endgeneration %}\")\n",
    "    \n",
    "    ## Dataset\n",
    "    def preprocess(batch):\n",
    "        def _single(messages):\n",
    "            enc = tok.apply_chat_template(messages, truncation=True, padding=\"max_length\", max_length=512)\n",
    "            ids, labels = enc[\"input_ids\"], [-100] * len(enc[\"input_ids\"])\n",
    "            if sep_id not in ids: return None\n",
    "            start = ids.index(sep_id) + 1\n",
    "            cand = [i for i in range(start, len(ids))\n",
    "                    if ids[i] not in (tok.pad_token_id, sep_id)]\n",
    "            if not cand: return None\n",
    "            # Each row we apply a random amount of noise (15-99%) and train as usual.\n",
    "            n_mask = max(int(len(cand) * random.uniform(0.15, 0.99)), 1)\n",
    "            for i in random.sample(cand, n_mask):\n",
    "                labels[i] = ids[i] # If masked, we apply loss on the token.\n",
    "                ids[i] = mask_id \n",
    "            return ids, labels\n",
    "            \n",
    "        mapped = (_single(m) for m in batch[\"messages\"])\n",
    "        filtered = (tup for tup in mapped if tup is not None)\n",
    "        ids, labels = zip(*filtered)\n",
    "        return {\"input_ids\": list(ids), \"labels\": list(labels)}\n",
    "    \n",
    "    ds = load_dataset(\"allenai/tulu-3-sft-mixture-0225\", split=\"train\")\n",
    "    dd = (ds\n",
    "        .map(preprocess, num_proc=32, batched=True, remove_columns=ds.column_names)\n",
    "        .train_test_split(0.05, seed=42))\n",
    "    \n",
    "    ## Train\n",
    "    project_name, run_name = \"dllm\", \"modernbert-dllm-tulu\"\n",
    "    os.environ.setdefault(\"WANDB_PROJECT\", run_name)\n",
    "    args = TrainingArguments(\n",
    "        run_name, bf16=True,\n",
    "        per_device_train_batch_size=32, per_device_eval_batch_size=32,\n",
    "        eval_strategy=\"steps\", eval_steps=200, num_train_epochs=1,\n",
    "        report_to=\"wandb\", push_to_hub=True,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model, args=args,\n",
    "        train_dataset=dd[\"train\"], eval_dataset=dd[\"test\"],\n",
    "    )\n",
    "    trainer.train()\n",
    "    tok.push_to_hub(f\"tommyp111/{run_name}\")\n",
    "\n",
    "notebook_launcher(main, num_processes=torch.cuda.device_count()) # DDP over number of ranks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f78908-6d67-4fa0-bae0-8d2af6b4f007",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "`[Prompt] [MASK] [MASK] [MASK] [MASK] ... [EOS]`\n",
    "\n",
    "### Iterative Denoising\n",
    "\n",
    "Unmask the token the model is most confident at each iteration step (greedy strategy).\n",
    "\n",
    "To speed up generation, we may unmask multiple tokens per step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795d9ade-bac1-4adf-b9f8-f2852328b107",
   "metadata": {},
   "source": [
    "### Block Generation\n",
    "\n",
    "One of the key limitations of diffusion inference is that the KV caching is not possible, since the attention mask is bi-directional all-to-all.\n",
    "\n",
    "This can be somewhat mitigated by generating \"blocks\" of tokens at a time, autoregressively.\n",
    "\n",
    "- E.g. When generating 128 tokens, autoregressively generate the first 32, add these to the KV cache and generate the next 32.\n",
    "- This also allows us to have a non-fixed response length.\n",
    "- A block length of 1 is somewhat equivalent to an autoregressive transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8546b4e8-b0bd-46dd-a735-304e289297a5",
   "metadata": {},
   "source": [
    "### Remasking & CFG\n",
    "\n",
    "Some more inference tricks:\n",
    "\n",
    "We can remask generated tokens to give the model another iteration at prediction.\n",
    "\n",
    "This can be done randomly, or remask low confidence tokens.\n",
    "\n",
    "---\n",
    "\n",
    "A form of \"CFG\" (classifier-free guidance).\n",
    "\n",
    "In this case this just passing only the paritially generated response without the prompt.\n",
    "\n",
    "It's unclear whether these optimizations helped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38e1f914-cdcc-44e9-9cb7-4d65330a6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "PROMPT_TEMPLATE = \"{cls_token_id} User: {user_prompt} {sep_token} Assistant:\\n\"\n",
    "\n",
    "def mk_assistant_message(tokenizer, answer_length, random_toks):\n",
    "    if random_toks:\n",
    "        assistant_init_ids = torch.randint(len(tokenizer.vocab), (answer_length,)).tolist()\n",
    "    else:\n",
    "        assistant_init_ids = [tokenizer.mask_token_id] * answer_length\n",
    "\n",
    "    return tokenizer.decode(assistant_init_ids)\n",
    "\n",
    "def mk_tokens(tokenizer, prompt, answer_length, random_toks):\n",
    "    # user_prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    if random_toks:\n",
    "        assistant_init_ids = torch.randint(len(tokenizer.vocab), (answer_length,)).tolist()\n",
    "    else:\n",
    "        assistant_init_ids = [tokenizer.mask_token] * answer_length\n",
    "\n",
    "    tokenizer.apply_chat_template([{\"content\": prompt}, {\"content\": tokenizer.decode(assistant_init_ids)}])\n",
    "    \n",
    "    # return user_prompt_ids, assistant_init_ids + [tokenizer.sep_token_id]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def iter_mask_decode(model, tokenizer, prompt: str, assistant_message, answer_length = 32, random_toks = False) -> str:\n",
    "    toks_dict = tokenizer.apply_chat_template(\n",
    "        [{\"content\": prompt}, {\"content\": assistant_message}],\n",
    "        return_dict=True, return_assistant_tokens_mask=True, return_tensors=\"pt\")\n",
    "    ids = toks_dict['input_ids'][0].tolist()\n",
    "    assistant_mask = toks_dict['assistant_masks']\n",
    "\n",
    "    answer_start =  assistant_mask[0].nonzero().min().item()\n",
    "    answer_end = answer_start + answer_length\n",
    "    device = next(model.parameters()).device\n",
    "    idxs = []\n",
    "    for step in range(answer_length):\n",
    "        logits = model(input_ids=torch.tensor([ids]).to(device)).logits\n",
    "        out_probs = torch.softmax(logits[0], dim=-1)\n",
    "        if random_toks:\n",
    "            candidate_locs = torch.arange(answer_start, answer_end)\n",
    "        else:\n",
    "            candidate_locs = (\n",
    "                (torch.tensor(ids) == tokenizer.mask_token_id)\n",
    "                .nonzero(as_tuple=True)[0]\n",
    "            )\n",
    "        \n",
    "        assert len(candidate_locs) != 0, \"out of masks\"\n",
    "        \n",
    "        candidate_probs = out_probs[candidate_locs]\n",
    "        candidate_max_probs = candidate_probs.max(dim=-1)[0]\n",
    "\n",
    "        # Argmax top prob\n",
    "        idx = candidate_max_probs.argmax()\n",
    "        pos = candidate_locs[idx]\n",
    "        new_token = candidate_probs[idx].argmax().item()\n",
    "        ids[pos] = new_token\n",
    "\n",
    "        # Token, Idx in answer mask\n",
    "        yield new_token, pos.item() - answer_start\n",
    "\n",
    "\n",
    "def iterative_print(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    user_prompt,\n",
    "    answer_length,\n",
    "    delay=0.0,\n",
    "    random_toks=False,\n",
    "):\n",
    "    def _print_step(resp, n_clear):\n",
    "        \"1) move to start, 2) blank the full width, 3) move back, 4) write new text\"\n",
    "        resp = resp.encode('unicode_escape').decode('ascii')\n",
    "        blank = \" \" * n_clear\n",
    "        sys.stdout.write(\"\\r\" + blank + \"\\r\" + resp)\n",
    "        sys.stdout.flush()\n",
    "        return len(resp)\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        cls_token_id=tokenizer.cls_token,\n",
    "        user_prompt=user_prompt,\n",
    "        sep_token=tokenizer.sep_token,\n",
    "    )\n",
    "    print(prompt, end=\"\")\n",
    "    \n",
    "    assistant_message = mk_assistant_message(tokenizer, answer_length, random_toks)\n",
    "    tok_iter = iter_mask_decode(model, tokenizer, prompt, assistant_message, answer_length, random_toks)\n",
    "    \n",
    "    # masked = tokenizer.convert_ids_to_tokens(tokenizer.encode(assistant_message))\n",
    "    \n",
    "    masked = [\"[MASK]\"] * answer_length # [\"[MASK]\"] * answer_length\n",
    "    n_clear = 0\n",
    "    for tok, pos in tok_iter:\n",
    "        masked[pos] = tokenizer.decode(tok)\n",
    "        # print(tokenizer.decode(tok))\n",
    "        resp = \"\".join(masked)\n",
    "        # print(pos, resp)\n",
    "        n_clear = _print_step(resp, n_clear)\n",
    "        time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcef5e57-5e9a-470a-a1ef-688c54a202da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] User: Who is the greatest soccer player? [SEP] Assistant:\n",
      "\\nThe greatest soccer player in history is the Brazilian football player, Ronaldinho Jr, who was born in 1986.[SEP].                                \n",
      " ************************* \n",
      "\n",
      "[CLS] User: What is the meaning of life? [SEP] Assistant:\n",
      "\\nThe meaning of life is to live, to love, to learn, to give, to share, to grow.[SEP]                                                             \n",
      " ************************* \n",
      "\n",
      "[CLS] User: What is the best place to go for a holiady? [SEP] Assistant:\n",
      "\\nThe best place to go for a holiady is a beach. The beach is full of sun and fun.[SEP]                                                              \n",
      " ************************* \n",
      "\n",
      "[CLS] User: x = 2y + 6; y = 3 * 4. Find x. [SEP] Assistant:\n",
      "\\nx = 2y + 6; y = 3 * 4.\\nThe solution is:\\nx = 2y[SEP]                                                                                              \n",
      " ************************* \n",
      "\n",
      "[CLS] User: What is singular value decomposition? [SEP] Assistant:\n",
      "\\nSingular value decomposition is a technique used in numerical analysis to separate the values of a function into smaller parts.\\n[SEP]                  \n",
      " ************************* \n",
      "\n",
      "[CLS] User: What is the captial of Australia? [SEP] Assistant:\n",
      "\\nThe captial of Australia is the name of the country of origin of the continent that formed the continent of Australia[SEP].                     \n",
      " ************************* \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, random, itertools, math, torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "device =  (\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "model_id = \"tommyp111/modernbert-dllm-tulu\" # https://wandb.ai/graphcore/modernbert-dllm-tulu/runs/nfblha6t?nw=nwusertompollak\n",
    "# model_id = \"tommyp111/modernbert-flowlm-tulu\" # https://wandb.ai/graphcore/dllm/runs/ldah4164?nw=nwusertompollak\n",
    "# model_id = \"answerdotai/ModernBERT-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device).eval()\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"Who is the greatest soccer player?\",\n",
    "    \"What is the meaning of life?\",\n",
    "    \"What is the best place to go for a holiady?\",\n",
    "    \"x = 2y + 6; y = 3 * 4. Find x.\",\n",
    "    \"What is singular value decomposition?\",\n",
    "    \"What is the captial of Australia?\",\n",
    "]\n",
    "for p in prompts:\n",
    "    iterative_print(model, tokenizer, p, answer_length=25, delay=0.1, random_toks=False)\n",
    "    print(\"\\n\", \"*\" * 25, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe00818-b3e0-42cb-8498-22035e8cadbc",
   "metadata": {},
   "source": [
    "# Diffusion Duality\n",
    "\n",
    "_\"The Diffusion Duality\" by Sahoo, Deschenaux et al. (arXiv: 2506.10892)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5381f6d2-565b-49f9-aaf6-f90f5f744fc0",
   "metadata": {},
   "source": [
    "Instead of `[MASK]`: Corrupt the input embeddings with noise, model is trained to denoise to the original embedding.\n",
    "\n",
    "- They argue that CEL with \"hard\" one-hot targets can lead to high-variance gradients, making training unstable when the input is very noisy.\n",
    "- The model predictions swing wildly between steps as it tries to achieve this certainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e61fce-54ea-4542-9dc6-cd4222617d2e",
   "metadata": {},
   "source": [
    "### Duo Corruption\n",
    "\n",
    "```python\n",
    "clean_weight = 1 - random.uniform(0.03, 0.15)\n",
    "noise_weight = sqrt(1 - clean_weight ** 2)\n",
    "\n",
    "hot = one_hot(input_ids) # (B,S,V)\n",
    "w = clean_weight * hot + noise_weight * random.randn_like(hot)\n",
    "soft_latents = softmax(w / temperature) # as temperature -> 0, this becomes the original one-hot token\n",
    "\n",
    "soft_embs = soft_latents @ model.W_embedding # (B,S,V) @ (V,D) -> (B,S,D)\n",
    "logits = model(input_embs=soft_embs)\n",
    "```\n",
    "\n",
    "Temperature is annealed from `1e-3` to `0` over the course of training\n",
    "\n",
    "This includes very large matmul of K=vocab size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf374b9e-3e29-4373-9675-7487cab7ae11",
   "metadata": {},
   "source": [
    "### Duo Loss\n",
    "\n",
    "```python\n",
    "p_log  = log_softmax(logits)\n",
    "p_prob = exp(p_log)\n",
    "\n",
    "q_prob = one_hot(target_ids)\n",
    "q_log  = log(q_prob)\n",
    "\n",
    "kl_pq  = kl_div(p_log, q_prob)      # (B,S,V)\n",
    "kl_qp  = kl_div(q_log, p_prob)      # (B,S,V)\n",
    "sym_kl = sum(kl_pq + kl_qp, dim=-1) # (B,S)\n",
    "\n",
    "# As clean_weight -> 1, view_scale -> 1, loss counts fully.\n",
    "view_scale = (1.0 - clean_weight) / (vocab_size * clean_weight + 1.0 - clean_weight)\n",
    "loss = (view_scale * sym_kl).sum()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
