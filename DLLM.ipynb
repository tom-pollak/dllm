{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d2405c-4cdb-4041-8ff7-b63183acf3bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Diffusion Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c6137",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are Language Diffusion Models?\n",
    "\n",
    "Instead of predicting the next token, a diffusion LLM must \"de-mask\" a corrupted sequence of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f9b25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LLaDA: Large Language Diffusion with mAsking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3964815",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The paper introduces **masked diffusion models**.\n",
    "\n",
    "These generate text by **iteratively denoising** a corrupted sequence.\n",
    "\n",
    "```\n",
    "[MASK] [MASK]  [MASK] [MASK] [MASK] [MASK] [EOS]\n",
    "[MASK] [MASK]  [MASK] France [MASK] [MASK] [EOS]\n",
    "The    [MASK]  [MASK] France [MASK] [MASK] [EOS]\n",
    "The    captial [MASK] France [MASK] [MASK] [EOS]\n",
    "The    captial [MASK] France is     [MASK] [EOS]\n",
    "The    captial [MASK] France is     Paris  [EOS]\n",
    "The    captial of     France is     Paris  [EOS]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfba0be",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In inference, this allows the model to produce multiple tokens per step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eab8d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "> **The core idea is very similar to BERT**\n",
    "\n",
    "We corrupt part of the input sequence with `[MASK]` tokens, and train the model to predict these tokens using cross-entropy loss (CEL)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea28113",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, while BERT is trained with a **fixed** 15% mask rate, LLaDA uses variable 15-99% masking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c34332-7e89-4f33-bf70-27df53d4c1a7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A disadvantage of BERT-like models: only get training signal from masked tokens.\n",
    "\n",
    "```\n",
    "The [MASK] of France [MASK] Paris [EOS]\n",
    "       ^                ^\n",
    "  Only the masked tokens contribute to the loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b85c51",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### A Minimal Training Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab350ee6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def iter_mask_decode(model, tokenizer, prompt: str, answer_length: int = 32):\n",
    "    # Create initial sequence with masked tokens\n",
    "    assistant_message = tokenizer.mask_token * answer_length\n",
    "    toks_dict = tokenizer.apply_chat_template(\n",
    "        [{\"content\": prompt}, {\"content\": assistant_message}],\n",
    "        return_dict=True, return_assistant_tokens_mask=True, return_tensors=\"pt\")\n",
    "    \n",
    "    ids = toks_dict['input_ids'][0].tolist()\n",
    "    assistant_mask = toks_dict['assistant_masks']\n",
    "    answer_start = assistant_mask[0].nonzero().min().item()\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    for step in range(answer_length):\n",
    "        logits = model(input_ids=torch.tensor([ids]).to(device)).logits\n",
    "        probs = torch.softmax(logits[0], dim=-1)\n",
    "\n",
    "        mask_positions = (torch.tensor(ids) == tokenizer.mask_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(mask_positions) == 0:\n",
    "            break\n",
    "\n",
    "        mask_probs = probs[mask_positions]\n",
    "        confidence_scores = mask_probs.max(dim=-1)[0]\n",
    "        best_idx = confidence_scores.argmax()\n",
    "\n",
    "        pos = mask_positions[best_idx]\n",
    "        new_token = mask_probs[best_idx].argmax().item()\n",
    "        ids[pos] = new_token\n",
    "        \n",
    "        yield new_token, pos.item() - answer_start\n",
    "\n",
    "def demo_inference(model, tokenizer, prompt: str, answer_length: int = 25, delay=0.1):\n",
    "    def _print_step(resp, n_clear):\n",
    "        \"\"\"1) move to start, 2) blank the full width, 3) move back, 4) write new text\"\"\"\n",
    "        resp = resp.encode('unicode_escape').decode('ascii')\n",
    "        blank = \" \" * n_clear\n",
    "        sys.stdout.write(\"\\r\" + blank + \"\\r\" + resp)\n",
    "        sys.stdout.flush()\n",
    "        return len(resp)\n",
    "    \n",
    "    print(f\"User: {prompt}\")\n",
    "    print(\"Assistant: \", end=\"\")\n",
    "    \n",
    "    tokens = [\"[MASK]\"] * answer_length\n",
    "    n_clear = 0\n",
    "    \n",
    "    for new_token, pos in iter_mask_decode(model, tokenizer, prompt, answer_length):\n",
    "        tokens[pos] = tokenizer.decode(new_token)\n",
    "        resp = \"\".join(tokens)\n",
    "        n_clear = _print_step(resp, n_clear)\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69135078",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os, random, itertools, math, torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "device =  (\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "model_id = \"tommyp111/modernbert-dllm-tulu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device).eval()\n",
    "\n",
    "prompt = \"What is the meaning of life?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f78908-6d67-4fa0-bae0-8d2af6b4f007",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inference\n",
    "\n",
    "> \"Greedy strategy\"\n",
    "\n",
    "Iteratively \"de-mask\" the most confident token at the most confidence point in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7b27cfb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is the meaning of life?\n",
      "\\nThe meaning of life is to live, to love, to laugh, to cry, to dream, to hope,[SEP]                                                              \n"
     ]
    }
   ],
   "source": [
    "demo_inference(model, tokenizer, prompt, answer_length=25, delay=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795d9ade-bac1-4adf-b9f8-f2852328b107",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Block Generation with KV cache.\n",
    "\n",
    "Key limitation: **KV caching is not possible**, since at the newly de-masked token attention is all-to-all.\n",
    "\n",
    "Mitigated by generating \"blocks\" of tokens at a time, autoregressively.\n",
    "\n",
    "E.g: To generate 128 tokens, we autoregressively generate block 4 blocks of 32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9eecd7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Remasking\n",
    "\n",
    "A key limitation of masked diffusion LMs is they have no **self-correction**\n",
    "\n",
    "The authors suggest confidence based remasking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe00818-b3e0-42cb-8498-22035e8cadbc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Diffusion Duality (Duo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a9e720",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Primary issue with masked diffusion models: No ability for self correction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7cce53",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For inference: Model starts from **random tokens**, that will be iteratively refined.\n",
    "\n",
    "- This allows it to self-correct it's previous predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5381f6d2-565b-49f9-aaf6-f90f5f744fc0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Duo's Approach:\n",
    "\n",
    "1. Corrupt one-hot tokens with a variable amount of gaussian noise.\n",
    "2. Convert back to one-hot (with argmax), some tokens will have flipped. This is our **corrupted sequence**.\n",
    "3. Train model to restore original sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bfde8b8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "input_ids = torch.arange(4)\n",
    "temperature = 1\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "def one_hot(x):\n",
    "    out = torch.nn.functional.one_hot(x)\n",
    "    print(f\"one_hot:\\n{out.numpy()}\\n\")\n",
    "    return out\n",
    "\n",
    "def softmax(x):\n",
    "    out = torch.nn.functional.softmax(x.to(torch.float32), dim=-1)\n",
    "    print(f\"sotmax:\\n{out.numpy()}\\n\")\n",
    "    return out\n",
    "\n",
    "def randn_like(x):\n",
    "    out = torch.randn_like(x.to(torch.float32))\n",
    "    print(f\"random noise:\\n{out.numpy()}\\n\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2a68d02",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot:\n",
      "[[1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n",
      "\n",
      "random noise:\n",
      "[[-0.18 -0.95  0.94  0.8 ]\n",
      " [ 0.03  0.75 -0.32 -0.17]\n",
      " [-1.3   0.49  1.63 -1.23]\n",
      " [ 0.52  0.05  0.06  0.13]]\n",
      "\n",
      "sotmax:\n",
      "[[0.34 0.14 0.27 0.26]\n",
      " [0.19 0.47 0.17 0.18]\n",
      " [0.11 0.2  0.58 0.11]\n",
      " [0.23 0.19 0.2  0.38]]\n",
      "\n",
      "noise: 0.364 | input: [0, 1, 2, 3] | corrupted: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "noise_weight = random.uniform(0.25, 0.5) # variable amount of noise\n",
    "clean_weight = 1 - noise_weight\n",
    "\n",
    "hot = one_hot(input_ids)\n",
    "w = clean_weight * hot + noise_weight * randn_like(hot)\n",
    "soft_latents = softmax(w / temperature)\n",
    "\n",
    "print(f\"noise: {noise_weight:.3f} | input: {input_ids.tolist()} | corrupted: {soft_latents.argmax(-1).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d97676",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"Soft\" Tokens\n",
    "\n",
    "To stabilize early training, we use a high temperature softmax to give a spread over the noisy tokens for a richer signal.\n",
    "\n",
    "Temperature is annealead to 0 over a course of 500K steps, where this becomes equivalent to `argmax`.\n",
    "\n",
    "```\n",
    "Early in training (temperature = 1e-3)\n",
    "softmax(noisy_one_hot_tokens / temperature) => [0.001, 0.92, 0.005, 0.074]\n",
    "\n",
    "Late training (temperature = 1e-8)\n",
    "softmax(noisy_one_hot_tokens / temperature) ~= argmax => [0, 1, 0, 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a3d1a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss\n",
    "\n",
    "By default, hard one-hot targets and CEL cause very high-variance gradiants, and it doesn't work very well.\n",
    "\n",
    "A.k.a: For two rows of 15% and 99% noise, the 99% is a much harder and will **dominate the gradient.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cbd0dc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Use a _Symmetric KL divergence_ between the target labels and logits.\n",
    "    - Similar to CEL, but also penalises over-predicting wrong labels.  \n",
    "- Use a scaling factor based on the amount of noise per row, so all rows have ~= loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf374b9e-3e29-4373-9675-7487cab7ae11",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "p_prob = softmax(logits)    # predicted\n",
    "q_prob = one_hot(input_ids) # ground-truth\n",
    "sym_kl = symmetric_kl_divergence(p_prob, q_prob)\n",
    "\n",
    "# As noise -> 0, view_scale -> 1, loss counts fully.\n",
    "view_scale = (noise_weight / (vocab_size * clean_weight + noise_weight)\n",
    "loss = (view_scale * sym_kl).sum()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
