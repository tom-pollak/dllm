{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d2405c-4cdb-4041-8ff7-b63183acf3bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Diffusion LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1311b5-0269-43ce-a0a8-c72193d2cbba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Current closed model: \n",
    "[https://chat.inceptionlabs.ai/](https://chat.inceptionlabs.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c03677f-5096-46b0-b0de-e1c07d0f451e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Large Language Diffusion Models (LLaDA)\n",
    "\n",
    "_\"Large Language Diffusion Models\" by Nie, Zhu, et al. (arXiv:2502.09992)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a3ff0-1c52-437a-a566-aa0a24160c8e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Proposes that \"_Generative modeling principles_, rather than the autoregressive formulation [...] fundamentally underpin the essential properties of LLMs\".\n",
    "\n",
    "The paper introduces **masked diffusion models** (MDM), bidirectional BERT-like models that operate on entire sequences in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b721e731-b1db-49b3-8ddf-91345aaccf8b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instead of predicting the next token, a diffusion LLM must \"de-mask\" a corrupted sequence of text.\n",
    "\n",
    "This is very similar to BERT! Except that we use a variable percentage of noise, rather than a fixed masking rate of 15%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f882db8-f4ef-4086-8964-3ea9d0578f22",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# ids[i] = (\n",
    "#     mask_id if dice < .8\n",
    "#     else random.randint(0, tok.vocab_size - 1) if dice < .9\n",
    "#     else ids[i]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f9bf4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abeb274",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given a sequence of text, \"corrupt\" a variable percentage with `[MASK]`. The model is trained to predict the original tokens that were masked using cross-entropy loss (CEL)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c34332-7e89-4f33-bf70-27df53d4c1a7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A major disadvantage of BERT-like models is that you only get training signal from masked tokens, unlike autoregressive models where every token contributes to the loss.\n",
    "\n",
    "```\n",
    "The [MASK] of France [MASK] Paris [EOS]\n",
    "       ^                ^\n",
    "  Only the masked tokens contribute to the loss\n",
    "```\n",
    "\n",
    "<!-- \n",
    "> For chosen tokens, BERT masks the token 80% of the time, adds a random vocab 10%, and keeps the token the same 10% of the time.\n",
    "\n",
    "This significantly reduces training efficiency compared to autoregressive models where all tokens predict the next token.\n",
    "\n",
    "This limitation exists because BERT-like models use bidirectional attention with no causal masking. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b85c51",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### A Minimal Training Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa45631-8d26-4bc4-8716-70558602ef17",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os, random, torch\n",
    "from datasets import load_dataset\n",
    "from accelerate import notebook_launcher\n",
    "from transformers import (AutoTokenizer, AutoModelForMaskedLM,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer)\n",
    "def main():\n",
    "    model_id = \"answerdotai/ModernBERT-large\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "    mask_id, sep_id, sep = tok.mask_token_id, tok.sep_token_id, tok.sep_token\n",
    "    tok.chat_template = (\n",
    "        \"User: {{ messages[0]['content'] }}\\n\" + sep + \"\\nAssistant:\\n{% generation %}{{ messages[1]['content'] }}{% endgeneration %}\")\n",
    "    \n",
    "    ## Dataset\n",
    "    def preprocess(batch):\n",
    "        def _single(messages):\n",
    "            enc = tok.apply_chat_template(messages, truncation=True, padding=\"max_length\", max_length=512)\n",
    "            ids, labels = enc[\"input_ids\"], [-100] * len(enc[\"input_ids\"])\n",
    "            if sep_id not in ids: return None\n",
    "            start = ids.index(sep_id) + 1\n",
    "            cand = [i for i in range(start, len(ids))\n",
    "                    if ids[i] not in (tok.pad_token_id, sep_id)]\n",
    "            if not cand: return None\n",
    "            # Each row we apply a random amount of noise (15-99%) and train as usual.\n",
    "            n_mask = max(int(len(cand) * random.uniform(0.15, 0.99)), 1)\n",
    "            for i in random.sample(cand, n_mask):\n",
    "                labels[i] = ids[i] # If masked, we apply loss on the token.\n",
    "                ids[i] = mask_id \n",
    "            return ids, labels\n",
    "            \n",
    "        mapped = (_single(m) for m in batch[\"messages\"])\n",
    "        filtered = (tup for tup in mapped if tup is not None)\n",
    "        ids, labels = zip(*filtered)\n",
    "        return {\"input_ids\": list(ids), \"labels\": list(labels)}\n",
    "    \n",
    "    ds = load_dataset(\"allenai/tulu-3-sft-mixture-0225\", split=\"train\")\n",
    "    dd = (ds\n",
    "        .map(preprocess, num_proc=32, batched=True, remove_columns=ds.column_names)\n",
    "        .train_test_split(0.05, seed=42))\n",
    "    \n",
    "    ## Train\n",
    "    project_name, run_name = \"dllm\", \"modernbert-dllm-tulu\"\n",
    "    os.environ.setdefault(\"WANDB_PROJECT\", run_name)\n",
    "    args = TrainingArguments(\n",
    "        run_name, bf16=True,\n",
    "        per_device_train_batch_size=32, per_device_eval_batch_size=32,\n",
    "        eval_strategy=\"steps\", eval_steps=200, num_train_epochs=1,\n",
    "        report_to=\"wandb\", push_to_hub=True,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model, args=args,\n",
    "        train_dataset=dd[\"train\"], eval_dataset=dd[\"test\"],\n",
    "    )\n",
    "    trainer.train()\n",
    "    tok.push_to_hub(f\"tommyp111/{run_name}\")\n",
    "\n",
    "notebook_launcher(main, num_processes=torch.cuda.device_count()) # DDP over number of ranks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f78908-6d67-4fa0-bae0-8d2af6b4f007",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3838930f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Iterative Denoising\n",
    "\n",
    "Unmask the token the model is most confident at each iteration step (greedy strategy).\n",
    "\n",
    "To speed up generation, we may unmask multiple tokens per step.\n",
    "\n",
    "\n",
    "`[Prompt] [MASK] [MASK] [MASK] [MASK] ... [EOS]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795d9ade-bac1-4adf-b9f8-f2852328b107",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Block Generation\n",
    "\n",
    "One of the key limitations of diffusion inference is that the KV caching is not possible, since the attention mask is bi-directional all-to-all.\n",
    "\n",
    "This can be somewhat mitigated by generating \"blocks\" of tokens at a time, autoregressively.\n",
    "\n",
    "- E.g. When generating 128 tokens, autoregressively generate the first 32, add these to the KV cache and generate the next 32.\n",
    "- This also allows us to have a non-fixed response length.\n",
    "- A block length of 1 is somewhat equivalent to an autoregressive transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8546b4e8-b0bd-46dd-a735-304e289297a5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Remasking & CFG\n",
    "\n",
    "Some more inference tricks:\n",
    "\n",
    "We can remask generated tokens to give the model another iteration at prediction.\n",
    "\n",
    "This can be done randomly, or remask low confidence tokens.\n",
    "\n",
    "---\n",
    "\n",
    "A form of \"CFG\" (classifier-free guidance).\n",
    "\n",
    "In this case this just passing only the partially generated response without the prompt.\n",
    "\n",
    "It's unclear whether these optimizations helped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38e1f914-cdcc-44e9-9cb7-4d65330a6c9c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def iter_mask_decode(model, tokenizer, prompt: str, answer_length: int = 32):\n",
    "    # Create initial sequence with masked tokens\n",
    "    assistant_message = tokenizer.mask_token * answer_length\n",
    "    toks_dict = tokenizer.apply_chat_template(\n",
    "        [{\"content\": prompt}, {\"content\": assistant_message}],\n",
    "        return_dict=True, return_assistant_tokens_mask=True, return_tensors=\"pt\")\n",
    "    \n",
    "    ids = toks_dict['input_ids'][0].tolist()\n",
    "    assistant_mask = toks_dict['assistant_masks']\n",
    "    answer_start = assistant_mask[0].nonzero().min().item()\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    for step in range(answer_length):\n",
    "        # Get model predictions\n",
    "        logits = model(input_ids=torch.tensor([ids]).to(device)).logits\n",
    "        probs = torch.softmax(logits[0], dim=-1)\n",
    "        \n",
    "        # Find remaining masked positions\n",
    "        mask_positions = (torch.tensor(ids) == tokenizer.mask_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(mask_positions) == 0:\n",
    "            break\n",
    "            \n",
    "        # Select most confident prediction\n",
    "        mask_probs = probs[mask_positions]\n",
    "        confidence_scores = mask_probs.max(dim=-1)[0]\n",
    "        best_idx = confidence_scores.argmax()\n",
    "        \n",
    "        # Unmask the most confident token\n",
    "        pos = mask_positions[best_idx]\n",
    "        new_token = mask_probs[best_idx].argmax().item()\n",
    "        ids[pos] = new_token\n",
    "        \n",
    "        yield new_token, pos.item() - answer_start\n",
    "\n",
    "def demo_inference(model, tokenizer, prompt: str, answer_length: int = 25):\n",
    "    \"\"\"Demonstrate iterative demasking with live output\"\"\"\n",
    "    def _print_step(resp, n_clear):\n",
    "        \"\"\"1) move to start, 2) blank the full width, 3) move back, 4) write new text\"\"\"\n",
    "        resp = resp.encode('unicode_escape').decode('ascii')\n",
    "        blank = \" \" * n_clear\n",
    "        sys.stdout.write(\"\\r\" + blank + \"\\r\" + resp)\n",
    "        sys.stdout.flush()\n",
    "        return len(resp)\n",
    "    \n",
    "    print(f\"User: {prompt}\")\n",
    "    print(\"Assistant: \", end=\"\")\n",
    "    \n",
    "    tokens = [\"[MASK]\"] * answer_length\n",
    "    n_clear = 0\n",
    "    \n",
    "    for new_token, pos in iter_mask_decode(model, tokenizer, prompt, answer_length):\n",
    "        tokens[pos] = tokenizer.decode(new_token)\n",
    "        resp = \"\".join(tokens)\n",
    "        n_clear = _print_step(resp, n_clear)\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    print()  # New line when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d10417",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os, random, itertools, math, torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "device =  (\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcef5e57-5e9a-470a-a1ef-688c54a202da",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Who is the greatest soccer player?\n",
      "\\nThe answer is:\\n****\\nA:\\nThe greatest soccer player is the Argentine footballer Lionel Messi.[SEP]                                             \n",
      "****************************************\n",
      "User: What is the meaning of life?\n",
      "\\nThe meaning of life is to live, to love, to laugh, to cry, to dream, to hope,[SEP]                                                              \n",
      "****************************************\n",
      "User: What is the best place to go for a holiday?\n",
      "\\nThe best place to go for a holiday is to visit a beautiful beach, a theme park, or a museum.[SEP]                                               \n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "model_id = \"tommyp111/modernbert-dllm-tulu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device).eval()\n",
    "\n",
    "prompts = [\n",
    "    \"Who is the greatest soccer player?\",\n",
    "    \"What is the meaning of life?\",\n",
    "    \"What is the best place to go for a holiday?\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    demo_inference(model, tokenizer, prompt, answer_length=25)\n",
    "    print(\"*\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe00818-b3e0-42cb-8498-22035e8cadbc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Diffusion Duality\n",
    "\n",
    "_\"The Diffusion Duality\" by Sahoo, Deschenaux et al. (arXiv: 2506.10892)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5381f6d2-565b-49f9-aaf6-f90f5f744fc0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Instead of `[MASK]`: Corrupt the input embeddings with noise, model is trained to denoise to the original embedding.\n",
    "\n",
    "- They argue that CEL with \"hard\" one-hot targets can lead to high-variance gradients, making training unstable when the input is very noisy.\n",
    "- The model predictions swing wildly between steps as it tries to achieve this certainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e61fce-54ea-4542-9dc6-cd4222617d2e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Duo Corruption\n",
    "\n",
    "```python\n",
    "clean_weight = 1 - random.uniform(0.03, 0.15)\n",
    "noise_weight = sqrt(1 - clean_weight ** 2)\n",
    "\n",
    "hot = one_hot(input_ids) # (B,S,V)\n",
    "w = clean_weight * hot + noise_weight * random.randn_like(hot)\n",
    "soft_latents = softmax(w / temperature) # as temperature -> 0, this becomes the original one-hot token\n",
    "\n",
    "soft_embs = soft_latents @ model.W_embedding # (B,S,V) @ (V,D) -> (B,S,D)\n",
    "logits = model(input_embs=soft_embs)\n",
    "```\n",
    "\n",
    "Temperature is annealed from `1e-3` to `0` over the course of training\n",
    "\n",
    "This includes very large matmul of K=vocab size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf374b9e-3e29-4373-9675-7487cab7ae11",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Duo Loss\n",
    "\n",
    "```python\n",
    "p_log  = log_softmax(logits)\n",
    "p_prob = exp(p_log)\n",
    "\n",
    "q_prob = one_hot(target_ids)\n",
    "q_log  = log(q_prob)\n",
    "\n",
    "kl_pq  = kl_div(p_log, q_prob)      # (B,S,V)\n",
    "kl_qp  = kl_div(q_log, p_prob)      # (B,S,V)\n",
    "sym_kl = sum(kl_pq + kl_qp, dim=-1) # (B,S)\n",
    "\n",
    "# As clean_weight -> 1, view_scale -> 1, loss counts fully.\n",
    "view_scale = (1.0 - clean_weight) / (vocab_size * clean_weight + 1.0 - clean_weight)\n",
    "loss = (view_scale * sym_kl).sum()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
