{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26997715-e998-4b39-9491-19168372ed65",
   "metadata": {},
   "source": [
    "# Large Language Diffusion Models (LLaDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31fc46e-d4c1-4cd3-a589-ac45995b30c8",
   "metadata": {},
   "source": [
    "The paper introduces **masked diffusion models**, bidrectional BERT-like models that operate \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85147a0-d55d-4389-99a8-a45c50aa6a97",
   "metadata": {},
   "source": [
    "## Diffusion Duality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30cd3cd3-9e4e-4950-9a47-5f990d8286d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50368"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff90998a-57a4-4e32-abcf-f35a43b5fa77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33986,\n",
       " 29625,\n",
       " 42119,\n",
       " 36341,\n",
       " 42868,\n",
       " 19619,\n",
       " 4407,\n",
       " 13624,\n",
       " 24001,\n",
       " 15302,\n",
       " 33402,\n",
       " 549,\n",
       " 42438,\n",
       " 3555,\n",
       " 32822,\n",
       " 465,\n",
       " 36802,\n",
       " 17610,\n",
       " 22496,\n",
       " 12877,\n",
       " 15244,\n",
       " 28122,\n",
       " 45055,\n",
       " 22231,\n",
       " 3860,\n",
       " 43852,\n",
       " 10751,\n",
       " 9378,\n",
       " 1102,\n",
       " 32589,\n",
       " 17177,\n",
       " 36561]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(len(tokenizer.vocab), (32,)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38e1f914-cdcc-44e9-9cb7-4d65330a6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "PROMPT_TEMPLATE = \"{cls_token_id} User: {user_prompt} {sep_token} Assistant:\\n\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def iter_mask_decode(model, tokenizer, prompt: str, answer_length = 32, random_toks = False) -> str:\n",
    "    user_prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    if random_toks:\n",
    "        assistant_init_ids = torch.randint(len(tokenizer.vocab), (answer_length,)).tolist()\n",
    "    else:\n",
    "        assistant_init_ids = [tokenizer.mask_token_id] * answer_length\n",
    "        \n",
    "    ids = (\n",
    "        user_prompt_ids\n",
    "        + assistant_init_ids\n",
    "        + [tokenizer.sep_token_id]\n",
    "    )\n",
    "\n",
    "    answer_start = len(user_prompt_ids)\n",
    "    answer_end = answer_start + answer_length\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    idxs = []\n",
    "    for step in range(answer_length):\n",
    "        logits = model(input_ids=torch.tensor([ids]).to(device)).logits\n",
    "        out_probs = torch.softmax(logits[0], dim=-1)\n",
    "        if random_toks:\n",
    "            mask_locs = torch.arange(answer_start, answer_end)\n",
    "        else:\n",
    "            mask_locs = (\n",
    "                (torch.tensor(ids) == tokenizer.mask_token_id)\n",
    "                .nonzero(as_tuple=True)[0]\n",
    "            )\n",
    "        \n",
    "        assert len(mask_locs) != 0, \"out of masks\"\n",
    "        \n",
    "        candidate_probs = out_probs[mask_locs]\n",
    "        candidate_max_probs = candidate_probs.max(dim=-1)[0]\n",
    "\n",
    "        # Argmax top prob\n",
    "        idx = candidate_max_probs.argmax()\n",
    "        pos = mask_locs[idx]\n",
    "        new_token = candidate_probs[idx].argmax().item()\n",
    "        ids[pos] = new_token\n",
    "\n",
    "        # Token, Idx in answer mask\n",
    "        yield new_token, pos.item() - answer_end\n",
    "\n",
    "def iterative_print(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    user_prompt,\n",
    "    answer_length,\n",
    "    delay=0.0,\n",
    "    random_toks=False,\n",
    "    decode_func=iter_mask_decode,\n",
    "):\n",
    "    def _print_step(resp, n_clear):\n",
    "        \"1) move to start, 2) blank the full width, 3) move back, 4) write new text\"\n",
    "        resp = resp.encode('unicode_escape').decode('ascii')\n",
    "        blank = \" \" * n_clear\n",
    "        sys.stdout.write(\"\\r\" + blank + \"\\r\" + resp)\n",
    "        sys.stdout.flush()\n",
    "        return len(resp)\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        cls_token_id=tokenizer.cls_token,\n",
    "        user_prompt=user_prompt,\n",
    "        sep_token=tokenizer.sep_token,\n",
    "    )\n",
    "    print(prompt, end=\"\")\n",
    "    \n",
    "    tok_iter = iter_mask_decode(model, tokenizer, prompt, answer_length, random_toks)\n",
    "    \n",
    "    masked = [\"[MASK]\"] * answer_length\n",
    "    n_clear = 0\n",
    "    for tok, pos in tok_iter:\n",
    "        masked[pos] = tokenizer.decode(tok)\n",
    "        # print(tokenizer.decode(tok))\n",
    "        resp = \"\".join(masked)\n",
    "        print(pos, resp)\n",
    "        n_clear = _print_step(resp, n_clear)\n",
    "        time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bcef5e57-5e9a-470a-a1ef-688c54a202da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, itertools, math, torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "device =  (\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "model_id = \"tommyp111/modernbert-flowlm\"\n",
    "# model_id = \"tommyp111/modernbert-diffusion\"\n",
    "# model_id = \"answerdotai/ModernBERT-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df5b5fa9-152c-4894-be2b-f08d764ff938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] User: What is the nature of life? [SEP] Assistant:\n",
      "-6 [MASK][MASK] of[MASK][MASK][MASK][MASK][MASK]\n",
      "[MASK][MASK] of[MASK][MASK][MASK][MASK][MASK]-6 [MASK][MASK] of[MASK][MASK][MASK][MASK][MASK]\n",
      "[MASK][MASK] of[MASK][MASK][MASK][MASK][MASK]-6 [MASK][MASK] of[MASK][MASK][MASK][MASK][MASK]\n",
      "[MASK][MASK] of[MASK][MASK][MASK][MASK][MASK]-6 [MASK][MASK] of[MASK][MASK][MASK][MASK][MASK]\n",
      "[MASK][MASK] of[MASK][MASK][MASK][MASK][MASK]-6 [MASK][MASK] of[MASK][MASK][MASK][MASK][MASK]\n",
      "[MASK][MASK] of[MASK][MASK][MASK][MASK][MASK]-6 [MASK][MASK] of[MASK][MASK][MASK][MASK][MASK]\n",
      "[MASK][MASK] of[MASK][MASK][MASK][MASK][MASK]-6 [MASK][MASK] of[MASK][MASK][MASK][MASK][MASK]\n",
      "[MASK][MASK] of[MASK][MASK][MASK][MASK][MASK]-6 [MASK][MASK] of[MASK][MASK][MASK][MASK][MASK]\n",
      "[MASK][MASK] of[MASK][MASK][MASK][MASK][MASK]"
     ]
    }
   ],
   "source": [
    "user_prompt = \"What is the nature of life?\"\n",
    "iterative_print(model, tokenizer, user_prompt, answer_length=8, delay=0.1, random_toks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094225c8-af45-416d-b344-107289fea6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Who is the greatest soccer player?\"\n",
    "for i in range(1, 16):\n",
    "    print(\"*\" * 10, i, \"*\" * 10)\n",
    "    iterative_print(model, tokenizer, user_prompt, answer_length=i, delay=0.0)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1aed68-3551-4905-8764-d42d3a735df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
