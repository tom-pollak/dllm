{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26997715-e998-4b39-9491-19168372ed65",
   "metadata": {},
   "source": [
    "# Large Language Diffusion Models (LLaDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31fc46e-d4c1-4cd3-a589-ac45995b30c8",
   "metadata": {},
   "source": [
    "The paper introduces **masked diffusion models** (MDM), bidrectional BERT-like models that operate on entire sequences in parallel.\n",
    "\n",
    "## Inference\n",
    "\n",
    "`[Prompt] [MASK] [MASK] [MASK] [MASK] [EOS]`\n",
    "\n",
    "At each time step the model can choose which token to demask (e.g. greedily unmask the token with the highest confidence in a token).\n",
    "\n",
    "- Can unmask multiple tokens per timestep, speeding up generation\n",
    "\n",
    "### Generating in blocks\n",
    "\n",
    "These can be somewhat mitigated by generating \"blocks\" of output autoregressively at a time -- aka generating the first 32 masked tokens, then the next 32. This allows us to have a non-fixed response length, and still use the KV cache for most of the output.\n",
    "\n",
    "You can think about a block length of 1 being a true autoregressive transformer.\n",
    "\n",
    "\n",
    "## Training\n",
    "\n",
    "Given a sequence of text, \"corrupt\" a variable percentage with `[MASK]`. Model is trained to predict the original tokens that were masked with CEL.\n",
    "\n",
    "- can remask randomly\n",
    "- They do \"CFG\" where they combine the probability of the token with no prompt.\n",
    "\n",
    "\n",
    "- Should loss be based on scaling ratio? 99% is much harder than 15%. I found the loss curve is very jagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb720f8-c6db-4440-8ef3-c2d3fcf8c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a874164d-ecf8-4956-ab89-69d3004b7291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "print(tok.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090d458-8325-40e5-b944-015ea7780460",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa45631-8d26-4bc4-8716-70558602ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, torch\n",
    "from datasets import load_dataset\n",
    "from accelerate import notebook_launcher\n",
    "from transformers import (AutoTokenizer, AutoModelForMaskedLM,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer)\n",
    "def main():\n",
    "    model_id = \"answerdotai/ModernBERT-large\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "    mask_id, sep_id, sep = tok.mask_token_id, tok.sep_token_id, tok.sep_token\n",
    "    tok.chat_template = (\n",
    "        \"User: {{ messages[0]['content'] }}\\n\" + sep + \"\\nAssistant:\\n{{ messages[1]['content'] }}\")\n",
    "    \n",
    "    ## Dataset\n",
    "    def preprocess(batch):\n",
    "        def _single(messages):\n",
    "            text = tok.apply_chat_template(messages, tokenize=False)\n",
    "            enc = tok(text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "            ids, labels = enc[\"input_ids\"], [-100] * len(enc[\"input_ids\"])\n",
    "            if sep_id not in ids: return None\n",
    "            start = ids.index(sep_id) + 1\n",
    "            cand = [i for i in range(start, len(ids))\n",
    "                    if ids[i] not in (tok.pad_token_id, sep_id)]\n",
    "            if not cand: return None\n",
    "            n_mask = max(int(len(cand) * random.uniform(0.15, 0.99)), 1)\n",
    "            for i in random.sample(cand, n_mask):\n",
    "                labels[i] = ids[i]\n",
    "                dice = random.random()\n",
    "                ids[i] = (\n",
    "                    mask_id if dice < .8\n",
    "                    else random.randint(0, tok.vocab_size - 1) if dice < .9\n",
    "                    else ids[i]\n",
    "                )\n",
    "            return ids, labels\n",
    "            \n",
    "        mapped = (_single(m) for m in batch[\"messages\"])\n",
    "        filtered = (tup for tup in mapped if tup is not None)\n",
    "        ids, labels = zip(*filtered)\n",
    "        return {\"input_ids\": list(ids), \"labels\": list(labels)}\n",
    "    \n",
    "    ds = load_dataset(\"allenai/tulu-3-sft-mixture-0225\", split=\"train\")\n",
    "    dd = (ds\n",
    "        .map(preprocess, num_proc=32, batched=True, remove_columns=ds.column_names)\n",
    "        .train_test_split(0.05, seed=42))\n",
    "    \n",
    "    ## Train\n",
    "    project_name, run_name = \"dllm\", \"modernbert-dllm-tulu\"\n",
    "    os.environ.setdefault(\"WANDB_PROJECT\", run_name)\n",
    "    args = TrainingArguments(\n",
    "        run_name,\n",
    "        per_device_train_batch_size=32, per_device_eval_batch_size=32,\n",
    "        bf16=True, remove_unused_columns=False,\n",
    "        eval_strategy=\"steps\", eval_steps=200, num_train_epochs=1,\n",
    "        report_to=\"wandb\", push_to_hub=True,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model, args=args,\n",
    "        train_dataset=dd[\"train\"], eval_dataset=dd[\"test\"],\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.push_to_hub()\n",
    "\n",
    "notebook_launcher(main, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85147a0-d55d-4389-99a8-a45c50aa6a97",
   "metadata": {},
   "source": [
    "## Diffusion Duality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "38e1f914-cdcc-44e9-9cb7-4d65330a6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "PROMPT_TEMPLATE = \"{cls_token_id} User: {user_prompt} {sep_token} Assistant:\\n\"\n",
    "\n",
    "\n",
    "def mk_tokens(tokenizer, prompt, answer_length, random_toks):\n",
    "    user_prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    if random_toks:\n",
    "        assistant_init_ids = torch.randint(len(tokenizer.vocab), (answer_length,)).tolist()\n",
    "    else:\n",
    "        assistant_init_ids = [tokenizer.mask_token_id] * answer_length\n",
    "\n",
    "    return user_prompt_ids, assistant_init_ids + [tokenizer.sep_token_id]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def iter_mask_decode(model, tokenizer, prompt: str, answer_length = 32, random_toks = False) -> str:\n",
    "    user_prompt_ids, assistant_init_ids = mk_tokens(tokenizer, prompt, answer_length, random_toks)\n",
    "    ids = user_prompt_ids + assistant_init_ids\n",
    "    \n",
    "    answer_start = len(user_prompt_ids)\n",
    "    answer_end = answer_start + answer_length\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    idxs = []\n",
    "    for step in range(answer_length):\n",
    "        logits = model(input_ids=torch.tensor([ids]).to(device)).logits\n",
    "        out_probs = torch.softmax(logits[0], dim=-1)\n",
    "        if random_toks:\n",
    "            candidate_locs = torch.arange(answer_start, answer_end)\n",
    "        else:\n",
    "            candidate_locs = (\n",
    "                (torch.tensor(ids) == tokenizer.mask_token_id)\n",
    "                .nonzero(as_tuple=True)[0]\n",
    "            )\n",
    "        \n",
    "        assert len(candidate_locs) != 0, \"out of masks\"\n",
    "        \n",
    "        candidate_probs = out_probs[candidate_locs]\n",
    "        candidate_max_probs = candidate_probs.max(dim=-1)[0]\n",
    "\n",
    "        # Argmax top prob\n",
    "        idx = candidate_max_probs.argmax()\n",
    "        pos = candidate_locs[idx]\n",
    "        new_token = candidate_probs[idx].argmax().item()\n",
    "        ids[pos] = new_token\n",
    "\n",
    "        # Token, Idx in answer mask\n",
    "        yield new_token, pos.item() - answer_start\n",
    "\n",
    "\n",
    "def iterative_print(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    user_prompt,\n",
    "    answer_length,\n",
    "    delay=0.0,\n",
    "    random_toks=False,\n",
    "    decode_func=iter_mask_decode,\n",
    "):\n",
    "    def _print_step(resp, n_clear):\n",
    "        \"1) move to start, 2) blank the full width, 3) move back, 4) write new text\"\n",
    "        resp = resp.encode('unicode_escape').decode('ascii')\n",
    "        blank = \" \" * n_clear\n",
    "        sys.stdout.write(\"\\r\" + blank + \"\\r\" + resp)\n",
    "        sys.stdout.flush()\n",
    "        return len(resp)\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        cls_token_id=tokenizer.cls_token,\n",
    "        user_prompt=user_prompt,\n",
    "        sep_token=tokenizer.sep_token,\n",
    "    )\n",
    "    print(prompt, end=\"\")\n",
    "    \n",
    "    tok_iter = iter_mask_decode(model, tokenizer, prompt, answer_length, random_toks)\n",
    "    \n",
    "    masked = [\"\"] * answer_length # [\"[MASK]\"] * answer_length\n",
    "    n_clear = 0\n",
    "    for tok, pos in tok_iter:\n",
    "        masked[pos] = tokenizer.decode(tok)\n",
    "        # print(tokenizer.decode(tok))\n",
    "        resp = \"\".join(masked)\n",
    "        # print(pos, resp)\n",
    "        n_clear = _print_step(resp, n_clear)\n",
    "        time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bcef5e57-5e9a-470a-a1ef-688c54a202da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, itertools, math, torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "device =  (\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "# model_id = \"tommyp111/modernbert-flowlm\"\n",
    "model_id = \"tommyp111/modernbert-diffusion\"\n",
    "# model_id = \"answerdotai/ModernBERT-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "df5b5fa9-152c-4894-be2b-f08d764ff938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] User: What is the nature of life? [SEP] Assistant:\n",
      "7\n",
      ".7\n",
      ".7\n",
      ".7\n",
      ".7\n",
      ".7\n",
      ".7\n",
      ".7\n",
      "."
     ]
    }
   ],
   "source": [
    "user_prompt = \"What is the nature of life?\"\n",
    "iterative_print(model, tokenizer, user_prompt, answer_length=8, delay=0.1, random_toks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "094225c8-af45-416d-b344-107289fea6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 1 **********\n",
      "[CLS] User: Who is the greatest soccer player? [SEP] Assistant:\n",
      "0\n",
      "Unknown\n",
      "********** 2 **********\n",
      "[CLS] User: Who is the greatest soccer player? [SEP] Assistant:\n",
      "0\n",
      "Michael1\n",
      "Michael Jordan\n",
      "********** 3 **********\n",
      "[CLS] User: Who is the greatest soccer player? [SEP] Assistant:\n",
      "0\n",
      "M2\n",
      "Minho1\n",
      "Mourinho\n",
      "********** 4 **********\n",
      "[CLS] User: Who is the greatest soccer player? [SEP] Assistant:\n",
      "3\n",
      ".0\n",
      "M.2\n",
      "Minho.1\n",
      "Mourinho.\n",
      "********** 5 **********\n",
      "[CLS] User: Who is the greatest soccer player? [SEP] Assistant:\n",
      "4\n",
      ".2\n",
      " Mess.3\n",
      " Messi.1\n",
      "el Messi.0\n",
      " Lionel Messi.\n",
      "********** 6 **********\n",
      "[CLS] User: Who is the greatest soccer player? [SEP] Assistant:\n",
      "5\n",
      ".1\n",
      " is.0\n",
      "This is.4\n",
      "This is question.2\n",
      "This is a question.3\n",
      "This is a subjective question.\n",
      "********** 7 **********\n",
      "[CLS] User: Who is the greatest soccer player? [SEP] Assistant:\n",
      "6\n",
      ".1\n",
      " is.0\n",
      "There is.2\n",
      "There is no.4\n",
      "There is no soccer.5\n",
      "There is no soccer player.3\n",
      "There is no greatest soccer player.\n",
      "********** 8 **********\n",
      "[CLS] User: Who is the greatest soccer player? [SEP] Assistant:\n",
      "7\n",
      ".0\n",
      "The.1\n",
      "The greatest.3\n",
      "The greatest player.2\n",
      "The greatest soccer player.4\n",
      "The greatest soccer player is.5\n",
      "The greatest soccer player is Michael.6\n",
      "The greatest soccer player is Michael Jordan.\n",
      "********** 9 **********\n",
      "[CLS] User: Who is the greatest soccer player? [SEP] Assistant:\n",
      "8\n",
      ".0\n",
      "The.1\n",
      "The greatest.3\n",
      "The greatest player.2\n",
      "The greatest soccer player.4\n",
      "The greatest soccer player is.5\n",
      "The greatest soccer player is Ronald.7\n",
      "The greatest soccer player is Ronaldela.6\n",
      "The greatest soccer player is Ronald Koela.\n",
      "********** 10 **********\n",
      "[CLS] User: Who is the greatest soccer player? [SEP] Assistant:\n",
      "9\n",
      ".0\n",
      "The.1\n",
      "The greatest.3\n",
      "The greatest player.2\n",
      "The greatest soccer player.4\n",
      "The greatest soccer player is.7\n",
      "The greatest soccer player is Mess.5\n",
      "The greatest soccer player is Lion Mess.6\n",
      "The greatest soccer player is Lionel Mess.8\n",
      "The greatest soccer player is Lionel Messi.\n",
      "********** 11 **********\n",
      "[CLS] User: Who is the greatest soccer player? [SEP] Assistant:\n",
      "10\n",
      ".0\n",
      "The.1\n",
      "The greatest.3\n",
      "The greatest player.2\n",
      "The greatest soccer player.4\n",
      "The greatest soccer player is.8\n",
      "The greatest soccer player is Mess.9\n",
      "The greatest soccer player is Messi.7\n",
      "The greatest soccer player isel Messi.6\n",
      "The greatest soccer player is Lionel Messi.5\n",
      "The greatest soccer player is undoubtedly Lionel Messi.\n",
      "********** 12 **********\n",
      "[CLS] User: Who is the greatest soccer player? [SEP] Assistant:\n",
      "11\n",
      ".0\n",
      "The.1\n",
      "The greatest.3\n",
      "The greatest player.2\n",
      "The greatest soccer player.4\n",
      "The greatest soccer player is.7\n",
      "The greatest soccer player is player.6\n",
      "The greatest soccer player is soccer player.5\n",
      "The greatest soccer player is American soccer player.10\n",
      "The greatest soccer player is American soccer playerham.9\n",
      "The greatest soccer player is American soccer player Beckham.8\n",
      "The greatest soccer player is American soccer player David Beckham.\n",
      "********** 13 **********\n",
      "[CLS] User: Who is the greatest soccer player? [SEP] Assistant:\n",
      "12\n",
      ".0\n",
      "The.1\n",
      "The greatest.3\n",
      "The greatest player.2\n",
      "The greatest soccer player.5\n",
      "The greatest soccer player all.4\n",
      "The greatest soccer player of all.6\n",
      "The greatest soccer player of all time.7\n",
      "The greatest soccer player of all time is.10\n",
      "The greatest soccer player of all time is Mess.8\n",
      "The greatest soccer player of all time is Lion Mess.9\n",
      "The greatest soccer player of all time is Lionel Mess.11\n",
      "The greatest soccer player of all time is Lionel Messi.\n",
      "********** 14 **********\n",
      "[CLS] User: Who is the greatest soccer player? [SEP] Assistant:\n",
      "13\n",
      ".0\n",
      "The.1\n",
      "The greatest.3\n",
      "The greatest player.2\n",
      "The greatest soccer player.5\n",
      "The greatest soccer player all.4\n",
      "The greatest soccer player of all.6\n",
      "The greatest soccer player of all time.7\n",
      "The greatest soccer player of all time is.8\n",
      "The greatest soccer player of all time is Sir.12\n",
      "The greatest soccer player of all time is Sirdo.10\n",
      "The greatest soccer player of all time is Sir Rdo.11\n",
      "The greatest soccer player of all time is Sir Ronaldo.9\n",
      "The greatest soccer player of all time is Siriano Ronaldo.\n",
      "********** 15 **********\n",
      "[CLS] User: Who is the greatest soccer player? [SEP] Assistant:\n",
      "14\n",
      ".0\n",
      "The.1\n",
      "The greatest.3\n",
      "The greatest player.2\n",
      "The greatest soccer player.5\n",
      "The greatest soccer player all.4\n",
      "The greatest soccer player of all.6\n",
      "The greatest soccer player of all time.7\n",
      "The greatest soccer player of all time is.8\n",
      "The greatest soccer player of all time is the.10\n",
      "The greatest soccer player of all time is the player.9\n",
      "The greatest soccer player of all time is the soccer player.13\n",
      "The greatest soccer player of all time is the soccer playerdo.11\n",
      "The greatest soccer player of all time is the soccer player Rdo.12\n",
      "The greatest soccer player of all time is the soccer player Ronaldo.\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"Who is the greatest soccer player?\"\n",
    "for i in range(1, 16):\n",
    "    print(\"*\" * 10, i, \"*\" * 10)\n",
    "    iterative_print(model, tokenizer, user_prompt, answer_length=i, delay=0.0)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1aed68-3551-4905-8764-d42d3a735df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d9c4f-8480-4e2d-aa0e-536e926d3608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
